{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "- [Importing Libraries and Dataset](#Importing-Libraries-and-Dataset)\n",
    "- [Functions for model presentation](#Functions-for-model-presentation)\n",
    "- [Training model](#Training-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mlxtend.preprocessing import DenseTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, make_scorer, recall_score, accuracy_score, precision_score, f1_score, roc_auc_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pd.read_csv('../datasets/val.csv')\n",
    "train = pd.read_csv('../datasets/train.csv')\n",
    "df = pd.read_csv('../datasets/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = val.text\n",
    "y_val = val.target_variable\n",
    "X_train = train.text\n",
    "y_train = train.target_variable\n",
    "X = df['text']\n",
    "y = df['target_variable']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for model presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our scorer based on accuracy_score\n",
    "scorers = {'precision_score': make_scorer(precision_score),\n",
    "           'recall_score': make_scorer(recall_score),\n",
    "           'accuracy_score': make_scorer(accuracy_score),\n",
    "           'f1_score': make_scorer(f1_score),\n",
    "           'roc_auc_score': make_scorer(roc_auc_score, needs_threshold=True)\n",
    "          }\n",
    "\n",
    "#make a function that prints evaluation metrics score\n",
    "def evaluation_metrics(model):\n",
    "    print('Train\\'s accuracy_score: {}'.format(round(model.score(X_train, y_train),4)))\n",
    "    print('Best accuracy score from training: {}'.format(round(model.best_score_,4)))\n",
    "    print('Validation\\'s accuracy score : {}'.format(round(model.score(X_val, y_val),4)))\n",
    "    print('Difference in accuracy scores between train and val: {}'.format(round(model.best_score_ - model.score(X_val, y_val),4)))\n",
    "    model_proba = [i[1] for i in model.predict_proba(X_val)]\n",
    "    print('ROC_AUC score on Validation Set: {}'.format(round(roc_auc_score(y_val, model_proba), 4)))\n",
    "    \n",
    "    y_pred = model.predict(X_val)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()\n",
    "    sensitivity = tp/(tp+fn)\n",
    "    specificity = tn/(tn+fp)\n",
    "    precision = tp/(tp+fp)\n",
    "    \n",
    "    print(f'Model sensitivity is : {sensitivity}')\n",
    "    print(f'Model specificity is : {specificity}')\n",
    "    print(f'Model f1 score is : {(2*sensitivity*precision)/(sensitivity+precision)}')\n",
    "    print('\\n\\nClassification report :\\n', classification_report(y_val, y_pred),'\\n')\n",
    "    print(pd.DataFrame({'Pred Negative' : [tn,fn], 'Pred Positive' : [fp,tp]}, index = ['Actual Negative','Actual Postitive']))\n",
    "\n",
    "\n",
    "#for final model section:\n",
    "#make a function that prints all classification metrics, AUC-ROC + TN, FP, FN, TP\n",
    "def all_metrics(model):\n",
    "    y_pred = model.predict(X_val)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()\n",
    "    print(\"True Negatives: \" + str(tn))\n",
    "    print(\"False Positives: \" + str(fp))\n",
    "    print(\"False Negatives: \" + str(fn))\n",
    "    print(\"True Positives: \" + str(tp))\n",
    "    print()\n",
    "    print('--------------------------------')\n",
    "    print()\n",
    "    print('Accuracy: {}'.format(round(accuracy_score(y_val, y_pred), 4)))\n",
    "    print('Misclassification rate: {}'.format(round((fp+fn)/(tp+fp+tn+fn),4)))\n",
    "    print('Precision: {}'.format(round(precision_score(y_val, y_pred), 4)))\n",
    "    print('Recall: {}'.format(round(recall_score(y_val, y_pred), 4)))\n",
    "    print('Specificity: {}'.format(round(tn/(tn+fp),4)))\n",
    "    print(f'Model f1 score is : {(f1_score(y_val, y_pred))}')\n",
    "    #get roc auc score\n",
    "    model_proba = [i[1] for i in model.predict_proba(X_val)]\n",
    "    print('ROC_AUC score on Validation Set: {}'.format(round(roc_auc_score(y_val, model_proba), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set stratified k-fold for cross validation.\n",
    "#we will use stratified k-fold since it is more suitable for binary classification.\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count Vectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pipe_cvec = Pipeline([('cvec',CountVectorizer()),('log',LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8271772967961475\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy : {np.mean(cross_val_score(log_pipe_cvec, X_train, y_train, cv = skf, n_jobs = -1))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TFIDF Vectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pipe_tvec = Pipeline([('cvec',CountVectorizer()),('tvec',TfidfTransformer()),('log',LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8295429209045556\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy : {np.mean(cross_val_score(log_pipe_tvec, X_train, y_train, cv = skf, n_jobs = -1))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 240 candidates, totalling 1200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  8.5min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 26.6min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed: 53.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1200 out of 1200 | elapsed: 88.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.9,\n",
       " 'cvec__max_features': 240000,\n",
       " 'cvec__min_df': 2,\n",
       " 'cvec__ngram_range': (1, 2),\n",
       " 'log__C': 10,\n",
       " 'log__penalty': 'l2',\n",
       " 'log__solver': 'liblinear'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'cvec__max_features': [5000, 8000, 10000, 16000, 240000],\n",
    "    'cvec__min_df': [2, 3],\n",
    "    'cvec__max_df': [0.9, 0.95],\n",
    "    'cvec__ngram_range': [(1,1),(1,2),(2,2)],\n",
    "    'log__penalty':['l1','l2'],\n",
    "    'log__solver':['liblinear'],\n",
    "    'log__C': [1,10]}\n",
    "\n",
    "log_gs_tvec = GridSearchCV(log_pipe_tvec, param_grid = params, cv=skf, n_jobs=-1, verbose=1, scoring=scorers, refit='accuracy_score')\n",
    "log_gs_tvec.fit(X_train, y_train)\n",
    "log_gs_tvec.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train's accuracy_score: 0.9882\n",
      "Best accuracy score from training: 0.8337\n",
      "Validation's accuracy score : 0.8349\n",
      "Difference in accuracy scores between train and val: -0.0011\n",
      "ROC_AUC score on Validation Set: 0.8978\n",
      "Model sensitivity is : 0.9018026565464896\n",
      "Model specificity is : 0.7152542372881356\n",
      "Model f1 score is : 0.8750287686996547\n",
      "\n",
      "\n",
      "Classification report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.72      0.76      1180\n",
      "           1       0.85      0.90      0.88      2108\n",
      "\n",
      "    accuracy                           0.83      3288\n",
      "   macro avg       0.83      0.81      0.82      3288\n",
      "weighted avg       0.83      0.83      0.83      3288\n",
      " \n",
      "\n",
      "                  Pred Negative  Pred Positive\n",
      "Actual Negative             844            336\n",
      "Actual Postitive            207           1901\n"
     ]
    }
   ],
   "source": [
    "evaluation_metrics(log_gs_tvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cvec',\n",
       "                 CountVectorizer(max_df=0.9, max_features=240000, min_df=2,\n",
       "                                 ngram_range=(1, 2))),\n",
       "                ('tvec', TfidfTransformer()),\n",
       "                ('log', LogisticRegression(C=10, solver='liblinear'))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = Pipeline([('cvec',CountVectorizer(max_df = 0.9,\n",
    "                                           max_features = 240000,\n",
    "                                           min_df = 2,\n",
    "                                           ngram_range = (1, 2))),\n",
    "                   ('tvec',TfidfTransformer()),\n",
    "                   ('log',LogisticRegression(C = 10,\n",
    "                                             penalty = 'l2',\n",
    "                                             solver = 'liblinear'))])\n",
    "logreg.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(logreg,open('../saved_models/logreg.sav','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
