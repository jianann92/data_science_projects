{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "- [Importing Libraries and Dataset](#Importing-Libraries-and-Dataset)\n",
    "- [Functions for model presentation](#Functions-for-model-presentation)\n",
    "- [Training model](#Training-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mlxtend.preprocessing import DenseTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, make_scorer, recall_score, accuracy_score, precision_score, f1_score, roc_auc_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pd.read_csv('../datasets/val.csv')\n",
    "train = pd.read_csv('../datasets/train.csv')\n",
    "df = pd.read_csv('../datasets/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = val.text\n",
    "y_val = val.target_variable\n",
    "X_train = train.text\n",
    "y_train = train.target_variable\n",
    "X = df['text']\n",
    "y = df['target_variable']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for model presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our scorer based on accuracy_score\n",
    "scorers = {'precision_score': make_scorer(precision_score),\n",
    "           'recall_score': make_scorer(recall_score),\n",
    "           'accuracy_score': make_scorer(accuracy_score),\n",
    "           'f1_score': make_scorer(f1_score),\n",
    "           'roc_auc_score': make_scorer(roc_auc_score, needs_threshold=True)\n",
    "          }\n",
    "\n",
    "#make a function that prints evaluation metrics score\n",
    "def evaluation_metrics(model):\n",
    "    print('Train\\'s accuracy_score: {}'.format(round(model.score(X_train, y_train),4)))\n",
    "    print('Best accuracy score from training: {}'.format(round(model.best_score_,4)))\n",
    "    print('Validation\\'s accuracy score : {}'.format(round(model.score(X_val, y_val),4)))\n",
    "    print('Difference in accuracy scores between train and val: {}'.format(round(model.best_score_ - model.score(X_val, y_val),4)))\n",
    "    model_proba = [i[1] for i in model.predict_proba(X_val)]\n",
    "    print('ROC_AUC score on Validation Set: {}'.format(round(roc_auc_score(y_val, model_proba), 4)))\n",
    "    \n",
    "    y_pred = model.predict(X_val)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()\n",
    "    sensitivity = tp/(tp+fn)\n",
    "    specificity = tn/(tn+fp)\n",
    "    precision = tp/(tp+fp)\n",
    "    \n",
    "    print(f'Model sensitivity is : {sensitivity}')\n",
    "    print(f'Model specificity is : {specificity}')\n",
    "    print(f'Model f1 score is : {(2*sensitivity*precision)/(sensitivity+precision)}')\n",
    "    print('\\n\\nClassification report :\\n', classification_report(y_val, y_pred),'\\n')\n",
    "    print(pd.DataFrame({'Pred Negative' : [tn,fn], 'Pred Positive' : [fp,tp]}, index = ['Actual Negative','Actual Postitive']))\n",
    "\n",
    "\n",
    "#for final model section:\n",
    "#make a function that prints all classification metrics, AUC-ROC + TN, FP, FN, TP\n",
    "def all_metrics(model):\n",
    "    y_pred = model.predict(X_val)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()\n",
    "    print(\"True Negatives: \" + str(tn))\n",
    "    print(\"False Positives: \" + str(fp))\n",
    "    print(\"False Negatives: \" + str(fn))\n",
    "    print(\"True Positives: \" + str(tp))\n",
    "    print()\n",
    "    print('--------------------------------')\n",
    "    print()\n",
    "    print('Accuracy: {}'.format(round(accuracy_score(y_val, y_pred), 4)))\n",
    "    print('Misclassification rate: {}'.format(round((fp+fn)/(tp+fp+tn+fn),4)))\n",
    "    print('Precision: {}'.format(round(precision_score(y_val, y_pred), 4)))\n",
    "    print('Recall: {}'.format(round(recall_score(y_val, y_pred), 4)))\n",
    "    print('Specificity: {}'.format(round(tn/(tn+fp),4)))\n",
    "    print(f'Model f1 score is : {(f1_score(y_val, y_pred))}')\n",
    "    #get roc auc score\n",
    "    model_proba = [i[1] for i in model.predict_proba(X_val)]\n",
    "    print('ROC_AUC score on Validation Set: {}'.format(round(roc_auc_score(y_val, model_proba), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set stratified k-fold for cross validation.\n",
    "#we will use stratified k-fold since it is more suitable for binary classification.\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count Vectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_pipe_cvec = Pipeline([('cvec',CountVectorizer()),('forest',RandomForestClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8105506859215381\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy : {np.mean(cross_val_score(forest_pipe_cvec, X_train, y_train, cv = skf, n_jobs = -1))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TFIDF Vectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_pipe_tvec = Pipeline([('cvec',CountVectorizer()),('tvec',TfidfTransformer()),('forest',RandomForestClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8060561183045578\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy : {np.mean(cross_val_score(forest_pipe_tvec, X_train, y_train, cv = skf, n_jobs = -1))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 18.5min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 42.1min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed: 75.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1080 out of 1080 | elapsed: 102.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.9,\n",
       " 'cvec__max_features': 60000,\n",
       " 'cvec__min_df': 2,\n",
       " 'cvec__ngram_range': (1, 2),\n",
       " 'forest__class_weight': 'balanced',\n",
       " 'forest__min_samples_leaf': 40,\n",
       " 'forest__n_estimators': 240,\n",
       " 'forest__random_state': 42}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'cvec__max_features': [24000, 40000, 60000],\n",
    "    'cvec__min_df': [2, 3],\n",
    "    'cvec__max_df': [0.9, 0.95],\n",
    "    'cvec__ngram_range': [(1,2)],\n",
    "    'forest__class_weight' : ['balanced', 'balanced_subsample'],\n",
    "    'forest__min_samples_leaf': [40, 45, 50],\n",
    "    'forest__n_estimators': [240, 260, 280],\n",
    "    'forest__random_state': [42]}\n",
    "\n",
    "forest_gs_cvec = GridSearchCV(forest_pipe_cvec, params, cv=skf, n_jobs=-1, verbose=1, scoring=scorers, refit='accuracy_score')\n",
    "forest_gs_cvec.fit(X_train, y_train)\n",
    "forest_gs_cvec.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train's accuracy_score: 0.7875\n",
      "Best accuracy score from training: 0.7803\n",
      "Validation's accuracy score : 0.785\n",
      "Difference in accuracy scores between train and val: -0.0046\n",
      "ROC_AUC score on Validation Set: 0.8487\n",
      "Model sensitivity is : 0.825426944971537\n",
      "Model specificity is : 0.7127118644067797\n",
      "Model f1 score is : 0.8311440171960831\n",
      "\n",
      "\n",
      "Classification report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.71      0.70      1180\n",
      "           1       0.84      0.83      0.83      2108\n",
      "\n",
      "    accuracy                           0.78      3288\n",
      "   macro avg       0.77      0.77      0.77      3288\n",
      "weighted avg       0.79      0.78      0.79      3288\n",
      " \n",
      "\n",
      "                  Pred Negative  Pred Positive\n",
      "Actual Negative             841            339\n",
      "Actual Postitive            368           1740\n"
     ]
    }
   ],
   "source": [
    "evaluation_metrics(forest_gs_cvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cvec',\n",
       "                 CountVectorizer(max_df=0.9, max_features=60000, min_df=2,\n",
       "                                 ngram_range=(1, 2))),\n",
       "                ('forest',\n",
       "                 RandomForestClassifier(class_weight='balanced',\n",
       "                                        min_samples_leaf=40, n_estimators=260,\n",
       "                                        random_state=42))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rforest = Pipeline([('cvec',CountVectorizer(max_df = 0.9,\n",
    "                                            max_features = 60000,\n",
    "                                            min_df = 2,\n",
    "                                            ngram_range = (1, 2))),\n",
    "                    ('forest',RandomForestClassifier(class_weight = 'balanced',\n",
    "                                                     min_samples_leaf = 40,\n",
    "                                                     n_estimators = 260,\n",
    "                                                     random_state = 42))])\n",
    "rforest.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(rforest,open('../saved_models/rforest.sav','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
