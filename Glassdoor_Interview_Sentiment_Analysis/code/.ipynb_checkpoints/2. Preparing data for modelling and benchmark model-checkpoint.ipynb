{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparing data for modelling and benchmark model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "- [Importing Libraries and Dataset](#Importing-Libraries-and-Dataset)\n",
    "- [Bench mark model](#Bench-mark-model)\n",
    "- [Baseline Score](#Baseline-Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jiana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jiana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "from matplotlib.pyplot import get_cmap\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import regex as re\n",
    "import requests\n",
    "from lxml import html\n",
    "import getpass\n",
    "from googletrans import Translator\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "import urllib\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import DenseTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, make_scorer, recall_score, accuracy_score, precision_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target_variable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one phone hour clear invited onsite five onsit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>phone done site four leader site take situatio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>best insanely fast easy maybe pandemic crisis ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sent personality simulator invited chime event...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pas virtual assessment scheduled two virtual d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32874</th>\n",
       "      <td>new office supposed operate dubai region posit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32875</th>\n",
       "      <td>position soon called clearly junior one maybe ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32876</th>\n",
       "      <td>first notified telephone waited long time fina...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32877</th>\n",
       "      <td>within day sent mail set video call one picked...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32878</th>\n",
       "      <td>happened video call since based china role mus...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32879 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  target_variable\n",
       "0      one phone hour clear invited onsite five onsit...                1\n",
       "1      phone done site four leader site take situatio...                1\n",
       "2      best insanely fast easy maybe pandemic crisis ...                1\n",
       "3      sent personality simulator invited chime event...                1\n",
       "4      pas virtual assessment scheduled two virtual d...                1\n",
       "...                                                  ...              ...\n",
       "32874  new office supposed operate dubai region posit...                0\n",
       "32875  position soon called clearly junior one maybe ...                0\n",
       "32876  first notified telephone waited long time fina...                0\n",
       "32877  within day sent mail set video call one picked...                0\n",
       "32878  happened video call since based china role mus...                0\n",
       "\n",
       "[32879 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32879 entries, 0 to 32878\n",
      "Data columns (total 2 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   text             32879 non-null  object\n",
      " 1   target_variable  32879 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 513.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    21079\n",
       "0    11800\n",
       "Name: target_variable, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target_variable.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['target_variable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.1, random_state=42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2108\n",
       "0    1180\n",
       "Name: target_variable, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    18971\n",
       "0    10620\n",
       "Name: target_variable, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving train and validation dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([X_train,y_train], axis = 1)\n",
    "val = pd.concat([X_val,y_val], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('../datasets/train.csv', index = False)\n",
    "val.to_csv('../datasets/val.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bench mark model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our scorer based on accuracy_score\n",
    "scorers = {'precision_score': make_scorer(precision_score),\n",
    "           'recall_score': make_scorer(recall_score),\n",
    "           'accuracy_score': make_scorer(accuracy_score),\n",
    "           'f1_score': make_scorer(f1_score),\n",
    "           'roc_auc_score': make_scorer(roc_auc_score, needs_threshold=True)\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our scorer based on accuracy_score\n",
    "scorers = {'precision_score': make_scorer(precision_score),\n",
    "           'recall_score': make_scorer(recall_score),\n",
    "           'accuracy_score': make_scorer(accuracy_score),\n",
    "           'f1_score': make_scorer(f1_score),\n",
    "           'roc_auc_score': make_scorer(roc_auc_score, needs_threshold=True)\n",
    "          }\n",
    "\n",
    "#make a function that prints evaluation metrics score\n",
    "def evaluation_metrics(model):\n",
    "    y_pred = model.predict(X_val)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()\n",
    "    sensitivity = tp/(tp+fn)\n",
    "    specificity = tn/(tn+fp)\n",
    "    precision = tp/(tp+fp)\n",
    "    train_acc = accuracy_score(y_train, model.predict(X_train))\n",
    "    val_acc = accuracy_score(y_pred, y_val)\n",
    "    \n",
    "    print('Train\\'s accuracy score : {}'.format(round(train_acc,5)))\n",
    "    print('Validation\\'s accuracy score : {}'.format(round(val_acc,5)))\n",
    "    print(f'Difference in accuracy scores between train and val: {round(train_acc-val_acc,5)}')\n",
    "    print(f'Model sensitivity is : {sensitivity}')\n",
    "    print(f'Model specificity is : {specificity}')\n",
    "    print(f'Model f1 score is : {(2*sensitivity*precision)/(sensitivity+precision)}')\n",
    "    model_proba = [i[1] for i in model.predict_proba(X_val)]\n",
    "    print('ROC_AUC score on Validation Set: {}'.format(round(roc_auc_score(y_val, model_proba), 4)))\n",
    "    print('\\n\\nClassification report :\\n', classification_report(y_val, y_pred),'\\n')\n",
    "    print(pd.DataFrame({'Pred Negative' : [tn,fn], 'Pred Positive' : [fp,tp]}, index = ['Actual Negative','Actual Postitive']))\n",
    "\n",
    "\n",
    "#for final model section:\n",
    "#make a function that prints all classification metrics, AUC-ROC + TN, FP, FN, TP\n",
    "def all_metrics(model):\n",
    "    y_pred = model.predict(X_val)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()\n",
    "    print(\"True Negatives: \" + str(tn))\n",
    "    print(\"False Positives: \" + str(fp))\n",
    "    print(\"False Negatives: \" + str(fn))\n",
    "    print(\"True Positives: \" + str(tp))\n",
    "    print()\n",
    "    print('--------------------------------')\n",
    "    print()\n",
    "    print('Accuracy: {}'.format(round(accuracy_score(y_val, y_pred), 4)))\n",
    "    print('Misclassification rate: {}'.format(round((fp+fn)/(tp+fp+tn+fn),4)))\n",
    "    print('Precision: {}'.format(round(precision_score(y_val, y_pred), 4)))\n",
    "    print('Recall: {}'.format(round(recall_score(y_val, y_pred), 4)))\n",
    "    print('Specificity: {}'.format(round(tn/(tn+fp),4)))\n",
    "    print(f'Model f1 score is : {(f1_score(y_val, y_pred))}')\n",
    "    #get roc auc score\n",
    "    model_proba = [i[1] for i in model.predict_proba(X_val)]\n",
    "    print('ROC_AUC score on Validation Set: {}'.format(round(roc_auc_score(y_val, model_proba), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cvec', CountVectorizer()), ('knn', KNeighborsClassifier())])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_pipe_cvec = Pipeline([('cvec',CountVectorizer()),('knn',KNeighborsClassifier())])\n",
    "knn_pipe_cvec.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.676094890510949"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_pipe_cvec.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train's accuracy score : 0.7534\n",
      "Validation's accuracy score : 0.67609\n",
      "Difference in accuracy scores between train and val: 0.07731\n",
      "Model sensitivity is : 0.9411764705882353\n",
      "Model specificity is : 0.20254237288135593\n",
      "Model f1 score is : 0.7883965825551361\n",
      "ROC_AUC score on Validation Set: 0.6402\n",
      "\n",
      "\n",
      "Classification report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.20      0.31      1180\n",
      "           1       0.68      0.94      0.79      2108\n",
      "\n",
      "    accuracy                           0.68      3288\n",
      "   macro avg       0.67      0.57      0.55      3288\n",
      "weighted avg       0.67      0.68      0.62      3288\n",
      " \n",
      "\n",
      "                  Pred Negative  Pred Positive\n",
      "Actual Negative             239            941\n",
      "Actual Postitive            124           1984\n"
     ]
    }
   ],
   "source": [
    "evaluation_metrics(knn_pipe_cvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 239\n",
      "False Positives: 941\n",
      "False Negatives: 124\n",
      "True Positives: 1984\n",
      "\n",
      "--------------------------------\n",
      "\n",
      "Accuracy: 0.6761\n",
      "Misclassification rate: 0.3239\n",
      "Precision: 0.6783\n",
      "Recall: 0.9412\n",
      "Specificity: 0.2025\n",
      "Model f1 score is : 0.7883965825551361\n",
      "ROC_AUC score on Validation Set: 0.6402\n"
     ]
    }
   ],
   "source": [
    "all_metrics(knn_pipe_cvec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
